{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Exercise_TF_Templogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PayamAkto/GEXERGY/blob/main/CNN_Exercise_TF_Templogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAcYJZopPA0n"
      },
      "source": [
        "# 1. Create synthetic temperature log data with three main scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmQwFOe0y-GW"
      },
      "source": [
        "## 1.1 load function for creating three temperature log scenarios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMObQPSDMWH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774434a7-627c-4fb0-a9ae-6afdd5714014"
      },
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "\n",
        "def CreateTrainingData5(n_m, n_x):\n",
        "    \n",
        "    MoveGaussian = np.arange(1, 20)\n",
        "    startT = 20   \n",
        "    maxTargetTemperature = 200\n",
        "    minTargeTemperature = 120\n",
        "    TargetTemperatures = np.round(minTargeTemperature+np.random.rand(n_m,1)*(maxTargetTemperature-minTargeTemperature))\n",
        "\n",
        "\n",
        "    maxPerturbLength = 0.50*n_x\n",
        "    minPerturbLength = 0.12*n_x\n",
        "    PerturbLengths = np.round(minPerturbLength+np.random.rand(n_m,1)*(maxPerturbLength-minPerturbLength))\n",
        "       \n",
        "    maxPerturbPoint = 0.9*n_x\n",
        "    minPerturbPoint = 0.3*n_x\n",
        "    startPerturbPoints = np.round(minPerturbPoint+np.random.rand(n_m,1)*(maxPerturbPoint-minPerturbPoint))\n",
        "    EndPerturbPoints = np.zeros((n_m,1))\n",
        "    \n",
        "    #pressure stuff\n",
        "    startP = 0.1 #MPa   \n",
        "    maxTargetP = 16\n",
        "    minTargeP = 20\n",
        "    TargetPressures = np.round(minTargeP+np.random.rand(n_m,1)*(maxTargetP-minTargeP))\n",
        "\n",
        "    TemperatureCurves = np.zeros([n_m, n_x])\n",
        "    PressureCurves = np.zeros([n_m, n_x])\n",
        "    ReservoirPressure = np.zeros([n_m, 1])\n",
        "    ReservoirPressureDepth = np.zeros([n_m, 1])\n",
        "    ReservoirTemperature = np.zeros([n_m, 1])\n",
        "    \n",
        "    for m in range(n_m):\n",
        "        s = int(startPerturbPoints[m])\n",
        "        e = int(startPerturbPoints[m]+PerturbLengths[m])\n",
        "        if(e>=n_x):\n",
        "            e=n_x-1\n",
        "        GeoGradLine = np.linspace(startT, TargetTemperatures[m], num=n_x)\n",
        "\n",
        "        StartT = GeoGradLine[s].copy() \n",
        "        IsoT = GeoGradLine[e].copy()\n",
        "       \n",
        "        randomFloat = np.random.rand()\n",
        "        if(randomFloat<0.33):#situation number 2\n",
        "            GeoGradLine[s:e] = GeoGradLine[e].copy()\n",
        "            startPtIdx_1 = s\n",
        "            endPtIdx_1 = int(s+np.random.uniform(0, 0.15)*(e-s))+1\n",
        "            m_slope_1 = (IsoT-StartT)/(endPtIdx_1-startPtIdx_1)\n",
        "            b_1 = IsoT - m_slope_1*endPtIdx_1\n",
        "\n",
        "            startPtIdx_2 = s\n",
        "            endPtIdx_2 = int(s+np.random.uniform(0.3, 0.7)*(e-s))\n",
        "            startPtT_2 = StartT + np.random.uniform(0.5, 0.85)*(IsoT-StartT)\n",
        "            m_slope_2 = (IsoT-startPtT_2)/(endPtIdx_2-startPtIdx_2)\n",
        "            b_2 = IsoT - m_slope_2*endPtIdx_2\n",
        "\n",
        "            x2 = np.arange(startPtIdx_2,endPtIdx_2)\n",
        "            l2 = (m_slope_2*x2+b_2).reshape(len(x2),1) \n",
        "            l1 = (m_slope_1*x2+b_1).reshape(len(x2),1) \n",
        "            x = np.min(np.hstack((l1, l2)), axis=1)\n",
        "            GeoGradLine[startPtIdx_2:endPtIdx_2] = x.reshape(-1,1)              \n",
        "            sigma = np.random.randint(1,3)\n",
        "            TemperatureCurves[m, :] = gaussian_filter(GeoGradLine[:,0], sigma=sigma)\n",
        "            startPerturbPoints[m] = int((b_2-b_1)/(m_slope_1-m_slope_2)+MoveGaussian[sigma])\n",
        "            if(e>=n_x-2):\n",
        "                 EndPerturbPoints[m] = e\n",
        "            else:\n",
        "                EndPerturbPoints[m] = e-int(MoveGaussian[sigma]/2)\n",
        "        elif(randomFloat<0.66): #situation number 1\n",
        "            GeoGradLine[s:e] = GeoGradLine[e].copy()\n",
        "            startPtIdx_1 = s\n",
        "            endPtIdx_1 = int(s+np.random.uniform(0.1, 0.5)*(e-s))\n",
        "            m_slope_1 = (IsoT-StartT)/(endPtIdx_1-startPtIdx_1)\n",
        "            b_1 = IsoT - m_slope_1*endPtIdx_1\n",
        "            x = np.arange(startPtIdx_1,endPtIdx_1)\n",
        "            GeoGradLine[startPtIdx_1:endPtIdx_1] = (m_slope_1*x+b_1).reshape(len(x),1)              \n",
        "            sigma = np.random.randint(1,3)\n",
        "            TemperatureCurves[m, :] = gaussian_filter(GeoGradLine[:,0], sigma=sigma)\n",
        "            startPerturbPoints[m] = endPtIdx_1+MoveGaussian[sigma]\n",
        "            if(e>=n_x-2):\n",
        "                 EndPerturbPoints[m] = e\n",
        "            else:\n",
        "                EndPerturbPoints[m] = e-MoveGaussian[sigma]\n",
        "        else:#situation number 3\n",
        "            s = int(np.random.uniform(0.5*n_x, 0.85*n_x))\n",
        "            e = n_x-1\n",
        "            GeoGradLine[s:e+1] = GeoGradLine[s].copy()\n",
        "            sigma = np.random.randint(1,15)\n",
        "            TemperatureCurves[m, :] = gaussian_filter(GeoGradLine[:,0], sigma=sigma)\n",
        "            startPerturbPoints[m] = s+MoveGaussian[sigma]\n",
        "            EndPerturbPoints[m] = e\n",
        "            \n",
        "            \n",
        "            \n",
        "        ReservoirTemperature[m] = GeoGradLine[e].copy()\n",
        "\n",
        "        PressureGradLine = np.linspace(startP, TargetPressures[m], num= n_x)\n",
        "        PressureCurves[m,:] = PressureGradLine.copy().reshape((n_x,))\n",
        "        midResPoint = int((startPerturbPoints[m]+EndPerturbPoints[m])/2)\n",
        "        ReservoirPressure[m] = PressureGradLine[midResPoint] \n",
        "        ReservoirPressureDepth[m] = (startPerturbPoints[m]+EndPerturbPoints[m])/2\n",
        "           \n",
        "    Y = np.hstack((startPerturbPoints, EndPerturbPoints, ReservoirTemperature, ReservoirPressure, ReservoirPressureDepth))\n",
        "    X = np.zeros((n_m, n_x, 2))\n",
        "    X[:, :, 0] = TemperatureCurves\n",
        "    X[:, :, 1] = PressureCurves\n",
        "    np.savetxt('TrainingDataX1.csv', X[:, :, 0], delimiter=',')\n",
        "    np.savetxt('TrainingDataX2.csv', X[:, :, 1], delimiter=',')\n",
        "    np.savetxt('TrainingDataY.csv', Y, delimiter=',')\n",
        "\n",
        "    return X, Y\n",
        "  \n",
        "print('congrats! you loaded the function into memory')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "congrats! you loaded the function into memory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNK0Yf8v0Pzu"
      },
      "source": [
        "## 1.2 Call function to make synthetic data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTBJC9WbPPvF",
        "outputId": "7b257f9e-11a4-436a-bc99-c2540403947e"
      },
      "source": [
        "# import plotting package\n",
        "import matplotlib.pyplot as plt\n",
        "# close any existing plots\n",
        "plt.close(\"all\")\n",
        "\n",
        "# how many points discretization per log?\n",
        "n_x=200\n",
        "well_depth = 2000\n",
        "dx = well_depth/n_x\n",
        "print('Discretization is: ' +str(dx))\n",
        "\n",
        "# how many training samples?\n",
        "n_m= 30000\n",
        "\n",
        "X, Y = CreateTrainingData5(n_m=n_m, n_x=n_x)\n",
        "print('Data is ready!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discretization is: 10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqr28yQ_EsDe"
      },
      "source": [
        "# 2. Attribute engineering: add the derivative"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iGtJRYuEwnt"
      },
      "source": [
        "print('data shape before added new attribute: ' + str(np.shape(X)))\n",
        "gradient = np.gradient(X[:, :, 0], axis=1) \n",
        "X = np.concatenate((X, gradient.reshape(n_m, n_x, 1)), axis=2)\n",
        "print('data shape after added new attribute: ' + str(np.shape(X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS9Eaeud2Qt6"
      },
      "source": [
        "## 1.3 plot the training data + new attribute --> derivative of the temperature log "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRoDGjJx2CTb"
      },
      "source": [
        "# the number of samples to visualize\n",
        "n_samples_viz = 20\n",
        "\n",
        "# Depth points\n",
        "Depth = np.arange(dx, dx*n_x+dx, dx)\n",
        "\n",
        "for sampleN in range(n_samples_viz):\n",
        "\n",
        "    f, axs = plt.subplots(nrows=1, ncols=3, sharey='row')\n",
        "\n",
        "    # plot the temperature log\n",
        "    axs[0].plot(X[sampleN, :,0], Depth, lw=1, alpha=1, color='r', label=r'Temperature log')\n",
        "    # plot the temperature feed zones\n",
        "    feedzone1Z = Y[sampleN, 0]\n",
        "    axs[0].plot([0, 280], [dx*feedzone1Z,dx*feedzone1Z], lw=1, alpha=1, color='b', label=r'Feed zone 1')\n",
        "    feedzone2Z = Y[sampleN, 1]\n",
        "    axs[0].plot([0, 280], [dx*feedzone2Z,dx*feedzone2Z], lw=1, alpha=1, color='b', label=r'Feed zone 2')\n",
        "    # plot text of the reservoir temperature\n",
        "    ResTemperature = Y[sampleN, 2]\n",
        "    axs[0].text(20, dx*feedzone2Z+100, 'Res Temp: ' + '{:.1f}'.format(ResTemperature), fontsize=12)\n",
        "    axs[0].set_xlim([min(X[sampleN, :,0]), max(X[sampleN, :,0])+10])\n",
        "\n",
        "    axs[0].set_xlabel(r'Temperature (C)')\n",
        "    axs[0].set_ylabel('Depth')\n",
        "    axs[0].set_title('Temperature')\n",
        "    axs[0].legend(bbox_to_anchor=(-1.65, 1.02), loc=\"upper left\")\n",
        "\n",
        "    # plot the derivative ofthe temperature log    \n",
        "    axs[1].plot(X[sampleN, :,2], Depth, lw=1, alpha=1, color='k', label=r'Temperature log')\n",
        "    axs[1].set_title('Temp derivative')\n",
        "\n",
        "    # plot the pressure log\n",
        "    axs[2].plot(X[sampleN, :, 1], Depth, lw=1, alpha=1, color='r', label=r'Pressure log')\n",
        "    PressureDepth = Y[sampleN, 4]*dx\n",
        "    ResPressure = Y[sampleN, 3]\n",
        "    axs[2].plot([0, 20], [PressureDepth,PressureDepth], lw=1, alpha=1, color='b', label=r'Pressure Depth Point')\n",
        "    axs[2].set_xlabel(r'Pressure (MP)')\n",
        "    axs[2].set_title('Pressure vs. depth')\n",
        "    axs[2].invert_yaxis()\n",
        "    axs[2].legend(bbox_to_anchor=(1.04, 1.02), loc=\"upper left\")\n",
        "    axs[2].text(10, PressureDepth-100, 'Res Pressure: ' + '{:.1f}'.format(ResPressure), fontsize=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kvnu3Mp87o2s"
      },
      "source": [
        "## 1.4 Normalize the input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugj6bTVo7mCy"
      },
      "source": [
        "# Compute and save the current mean and standard deviation (std) values\n",
        "Xmean = np.mean(X, axis=1)\n",
        "Xstd = np.std(X, axis=1)  \n",
        "np.savetxt('Xmean.csv', Xmean, delimiter=',')\n",
        "np.savetxt('Xstd.csv', Xstd, delimiter=',')  \n",
        "\n",
        "# Normalize the input data \n",
        "Xmean = np.reshape(Xmean, (n_m, 1, 3))\n",
        "Xstd = np.reshape(Xstd, (n_m, 1, 3))    \n",
        "X = (X-Xmean)/Xstd\n",
        "#RangeTemp = Xmax[:,:,0]-Xmin[:,:,0]\n",
        "#RangePressure = Xmax[:,:,1]-Xmin[:,:,1]\n",
        "#MinTemp = Xmin[:,:,0]\n",
        "#MinPressure = Xmin[:,:,1]\n",
        "print('done loading and normalizing data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6jPk0x8Crn_"
      },
      "source": [
        "# 2. Split the data into training, validation, and testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw7J2kgtCCxK"
      },
      "source": [
        "ratio_train = 0.8\n",
        "ratio_val = 0.1\n",
        "ratio_test = 0.1\n",
        "n_train = int(ratio_train*n_m)\n",
        "n_val = int(ratio_val*n_m)\n",
        "n_test = n_m-n_val-n_train\n",
        "\n",
        "print(str(n_train) + ' samples in training')\n",
        "print(str(n_val) + ' samples in validation')\n",
        "print(str(n_test) + ' samples in testing')\n",
        "\n",
        "XTrain = X[0:n_train]\n",
        "YTrain = Y[0:n_train]\n",
        "\n",
        "XVal = X[n_train:n_train+n_val]\n",
        "YVal = Y[n_train:n_train+n_val]\n",
        "\n",
        "XTest = X[n_train+n_val:]\n",
        "YTest = Y[n_train+n_val:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZrDua4mRQ7V"
      },
      "source": [
        "#3. Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y69PZDq8DRzr"
      },
      "source": [
        "## 3.1 load lots of packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMTRUTFMUZYi"
      },
      "source": [
        "# import machine learning models\n",
        "import os\n",
        "import multiprocessing\n",
        "from multiprocessing import Lock, Process, Queue, current_process\n",
        "import time\n",
        "import queue # imported for using queue.Empty exception\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import random   \n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Conv1D, MaxPooling1D, Dense\n",
        "from keras.layers import Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.framework import ops\n",
        "import math\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from keras.models import load_model\n",
        "\n",
        "%load_ext tensorboard\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "run_n = 0\n",
        "print('finished loading packages')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVNJVW5QDaU5"
      },
      "source": [
        "## 3.2 load overarching architecture function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXcdK4jIRTdH"
      },
      "source": [
        "# =============================================================================\n",
        "# This section is where the CNN model is defined\n",
        "# =============================================================================\n",
        "def conv_block_first(model, params):\n",
        "    \"\"\"\n",
        "    The first convolutional block in each architecture. \n",
        "    Only separate from the other blocks so we can specify the input shape.\n",
        "    \"\"\"    \n",
        "\n",
        "    #First Stacked Convolution\n",
        "    model.add(Conv1D(filters = params['Conv1D_1_output_filters'], \n",
        "                     kernel_size = params['Conv1D_1_kernel_size'], \n",
        "                     padding = params['Conv1D_1_padding'], \n",
        "                     input_shape = params[\"input_shape\"]))\n",
        "\n",
        "    if params['Conv1D_1_batchnorm']:\n",
        "        model.add(BatchNormalization())\n",
        "        \n",
        "    model.add(Activation(params['Conv1D_1_activation']))\n",
        "    \n",
        "    if params['Conv1D_1_maxpool']:\n",
        "        model.add(MaxPooling1D(pool_size=params['Conv1D_1_maxpool_size'], \n",
        "                               padding = params['Conv1D_1_maxpool_padding'], \n",
        "                               data_format='channels_last'))\n",
        "\n",
        "    if params['Conv1D_1_dropout']:\n",
        "        model.add(Dropout(params['Conv1D_1_dropout_rate']))\n",
        "    return model\n",
        "\n",
        "def conv_block(model, params):\n",
        "    \"\"\"\n",
        "    Generic convolutional block, including:\n",
        "      - convolutions, \n",
        "      - max pooling, \n",
        "      - dropout, \n",
        "      - and an optional Batch Normalization.\n",
        "    \"\"\"\n",
        "    model.add(Conv1D(filters = params['Conv1D_mid_output_filters'], \n",
        "                     kernel_size = params['Conv1D_mid_kernel_size'], \n",
        "                     padding = params['Conv1D_mid_padding']))\n",
        "    \n",
        "    if params['Conv1D_mid_batchnorm']:\n",
        "        model.add(BatchNormalization())\n",
        "    \n",
        "    model.add(Activation(params['Conv1D_mid_activation']))\n",
        "\n",
        "    if params['Conv1D_mid_maxpool']:      \n",
        "        model.add(MaxPooling1D(pool_size=params['Conv1D_mid_maxpool_size'],\n",
        "                               padding = params['Conv1D_mid_maxpool_padding'],\n",
        "                               data_format='channels_last'))\n",
        "    \n",
        "    if params['Conv1D_mid_dropout']: \n",
        "        model.add(Dropout(params['Conv1D_mid_dropout_rate']))\n",
        "    \n",
        "    return model\n",
        "\n",
        "def fn_block(model):\n",
        "    \"\"\"\n",
        "    The final block.\n",
        "    \"\"\"\n",
        "    model.add(Flatten())\n",
        "    # we have here dense of 5 because we are expecting five outputs\n",
        "    model.add(Dense(5))\n",
        "    return model\n",
        "\n",
        "def build_model(params):\n",
        "    \"\"\"\n",
        "    Builds a sequential network based on the specified parameters.\n",
        "    \n",
        "    blocks: number of convolutional blocks in the network, must be greater than 2.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    model = conv_block_first(model, params)\n",
        "\n",
        "    for block in range(1,params['num_conv_blocks']):\n",
        "        model = conv_block(model, params)\n",
        "\n",
        "    model = fn_block(model)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wacvBa9KJ8oj"
      },
      "source": [
        "## 3.3 Set the hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gl7zPYWoeULz"
      },
      "source": [
        "#set the ranges for the hyper-parameter space\n",
        "start = time.time()\n",
        "params= {'Conv1D_1_batchnorm': 'True', 'Conv1D_1_activation':'relu', 'Conv1D_1_maxpool': 'True', 'Conv1D_1_maxpool_padding': 'same', \n",
        "         'Conv1D_1_dropout': 'False', 'Conv1D_mid_padding':'same', 'Conv1D_mid_batchnorm':'True', 'Conv1D_mid_maxpool_padding': 'same',\n",
        "         'Conv1D_mid_dropout': 'True', 'Conv1D_mid_maxpool':'True', 'Conv1D_1_dropout_rate': 0.1, 'Conv1D_mid_dropout_rate': 0.1, \n",
        "         'decay': 0.0001, 'Conv1D_1_output_filters': 19, 'batch_size': 100, 'batch_size': 100, 'Conv1D_mid_kernel_size': 3}\n",
        "\n",
        "##########################\n",
        "# Discrete parameters\n",
        "# divided into parameters for the first block, middle blocks, and final block\n",
        "##########################\n",
        "\n",
        "# convolution filter edge treatment for first convolution block\n",
        "# 'Conv1D_1_padding':['valid', 'same']\n",
        "params['Conv1D_1_padding'] = 'valid'; \n",
        "\n",
        "# The activation used for the first convolution block\n",
        "# 'Conv1D_mid_activation': ['relu', 'sigmoid', 'tanh']\n",
        "params['Conv1D_mid_activation'] = 'relu'\n",
        "\n",
        "###############################\n",
        "# float values\n",
        "##############################\n",
        "\n",
        "# 'learning_rate':[0.001, 0.007]\n",
        "params['learning_rate'] = 0.002\n",
        "\n",
        "###############################\n",
        "# integer parameters\n",
        "# shown with the min and max recommended values\n",
        "##############################\n",
        "\n",
        "# The number of covolution blocks\n",
        "# 'num_conv_blocks': [2, 7]\n",
        "params['num_conv_blocks'] = 3\n",
        "\n",
        "# The kernel/filter size for the first convolutional block\n",
        "# 'Conv1D_1_kernel_size':[3, 30] \n",
        "params['Conv1D_1_kernel_size'] = 7\n",
        "\n",
        "# The number of filters in the middle convolution blocks\n",
        "# 'Conv1D_mid_output_filters': [3, 35]\n",
        "params['Conv1D_mid_output_filters'] = 16\n",
        "\n",
        "# the size of the kernel in the first maxpooling\n",
        "# 'Conv1D_1_maxpool_size':[2, 4]\n",
        "params['Conv1D_1_maxpool_size'] = 3\n",
        "\n",
        "# the size of the kernel in the middle blocks\n",
        "# 'Conv1D_mid_maxpool_size':[2, 4]\n",
        "params['Conv1D_mid_maxpool_size'] = 3\n",
        "\n",
        "# The filter size in the middle convolution block\n",
        "# 'Conv1D_mid_kernel_size': [3, 30]\n",
        "params['Conv1D_mid_kernel_size'] = 3\n",
        "\n",
        "print('you finished setting the hyper parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-NX9oBEKCir"
      },
      "source": [
        "## 3.4 create the CNN model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzdsyZ0FeDr1"
      },
      "source": [
        "# determine the data input shape\n",
        "input_shape = XTrain.shape[1:]\n",
        "params[\"input_shape\"] = input_shape\n",
        "\n",
        "# build the model\n",
        "model = build_model(params)\n",
        "\n",
        "# create an optimizer object\n",
        "optimizer = keras.optimizers.Adam(lr=params['learning_rate'],\n",
        "                                  beta_1=0.9, \n",
        "                                  beta_2=0.999, \n",
        "                                  clipnorm=1, \n",
        "                                  decay=params['decay'])\n",
        "\n",
        "# compile the model and set the tracking parameters\n",
        "model.compile(optimizer = optimizer,\n",
        "              loss = \"mean_squared_error\",\n",
        "              metrics = [tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "# view the model summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Psnb3p5MGHR"
      },
      "source": [
        "## 3.5 run the model! Loop over 4 options of values for the number of convolution blocks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Hru7PoZjJt0"
      },
      "source": [
        "check_out_num_blocks = [0, 1, 2, 3]\n",
        "histories = [0,0,0,0]\n",
        "for i in range(4):\n",
        "\n",
        "    params['num_conv_blocks'] = check_out_num_blocks[i]\n",
        "\n",
        "    params['filename'] = 'run_'+str(i) + '_' + str(check_out_num_blocks[i])+'_blocks'\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(params['filename']+'.h5', \n",
        "                                      monitor='val_mean_absolute_error',\n",
        "                                    save_best_only = True)\n",
        "\n",
        "\n",
        "    log_dir = 'tb_logs/'+params['filename']\n",
        "    tb = TensorBoard(log_dir=log_dir)\n",
        "    file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "    file_writer.set_as_default()\n",
        "\n",
        "    hparams = {i:params[i] for i in params if i in ['Conv1D_mid_kernel_size', 'num_conv_blocks', 'learning_rate']}\n",
        "    histories[i] = model.fit(x=XTrain, y=YTrain,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=15, \n",
        "                        verbose=1, \n",
        "                        callbacks=[model_checkpoint, tb, hp.KerasCallback(log_dir, hparams, trial_id=params['filename'])], \n",
        "                        validation_data=(XVal, YVal), \n",
        "                        shuffle=True,\n",
        "                        class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "    \n",
        "    runName = str(params['filename']) + '.h5'\n",
        "    new_model = load_model(runName)\n",
        "    Prediction = new_model.predict(XVal)\n",
        "    mean_abs_error = np.mean(np.abs(Prediction - YVal))\n",
        "\n",
        "    tf.summary.scalar('validation_mae', data=mean_abs_error, step=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q45nMwSVSN8m"
      },
      "source": [
        "## 3.6 Plot the error decrease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjqvAQGMSLcw"
      },
      "source": [
        "history = histories[0] \n",
        "\n",
        "f = plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.plot(history.history['mean_absolute_error'], label ='training mean abs err')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='validation mean abs err')\n",
        "plt.ylabel('mean_absolute_error')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "f.savefig('val_mean_absolute_error.png',dpi=300,bbox_inches='tight')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LUpcVQS-M9"
      },
      "source": [
        "## 3.7 plot with tensor board"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3-lvgqDS5kO"
      },
      "source": [
        "%tensorboard --'logdir' 'tb_logs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTaFyDRzdE4b"
      },
      "source": [
        "# 4. Evaluate results on testing set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnM_q45IMkvn"
      },
      "source": [
        "runName = str(params['filename']) + '.h5'\n",
        "new_model = load_model(runName)\n",
        "Prediction = new_model.predict(XTest)\n",
        "\n",
        "mean_abs_error = np.mean(np.abs(Prediction - YTest))\n",
        "print('Testing mean absolute error is: ' + str(mean_abs_error))\n",
        "\n",
        "#reset Truth and Pred Y\n",
        "rangeDepth=10\n",
        "minDepth=0\n",
        "\n",
        "Depth = np.linspace(dx, n_x*dx, n_x)\n",
        "alpha = 0.8\n",
        "lw = 2\n",
        "Colors = ['r', 'g', 'b', 'k', 'y']\n",
        "Depth_i = np.linspace(0, 2000, n_x)\n",
        "absError = np.zeros((12,1))\n",
        "\n",
        "txt_x = -0.7\n",
        "txt_y = 250\n",
        "\n",
        "for n in range(20):\n",
        "    f, axs = plt.subplots(1, 2, sharey='row')\n",
        "\n",
        "    # plot temperature \n",
        "    axs[0].plot(XTest[n, :, 0], Depth_i, lw=lw, alpha=alpha, color='r', \n",
        "                label=r'Log')\n",
        "\n",
        "    # plot feed zones - truth \n",
        "    feedzone1Z = dx*YTest[n, 0]\n",
        "    axs[0].plot([np.min(XTest[n, :, 0]), np.max(XTest[n, :, 0])],\n",
        "                [feedzone1Z,feedzone1Z], \n",
        "                lw=lw, alpha=alpha, color='g', label=r'Truth - Feed zone 1')\n",
        "    feedzone2Z = dx*YTest[n, 1]\n",
        "    axs[0].plot([np.min(XTest[n, :, 0]), np.max(XTest[n, :, 0])],\n",
        "                [feedzone2Z,feedzone2Z], lw=lw, alpha=alpha, color='g',\n",
        "                label=r'Truth - Feed zone 2')\n",
        "\n",
        "    # plot feed zones - prediction \n",
        "    feedzone1Z_p = dx*Prediction[n, 0]\n",
        "    axs[0].plot([np.min(XTest[n, :, 0]), np.max(XTest[n, :, 0])],\n",
        "                [feedzone1Z_p,feedzone1Z_p], lw=lw, alpha=alpha, \n",
        "                color='b',linestyle='dashed', label=r'Preiction - Feed zone 1')\n",
        "    feedzone2Z_p = dx*Prediction[n, 1]\n",
        "    axs[0].plot([np.min(XTest[n, :, 0]), np.max(XTest[n, :, 0])],\n",
        "                [feedzone2Z_p,feedzone2Z_p], lw=lw, alpha=alpha, \n",
        "                color='b',linestyle='dashed', label=r'Prediction - Feed zone 2')\n",
        "\n",
        "    # plot reservoir temperature truth vs. prediction\n",
        "    ResTempT = YTest[n, 2]\n",
        "    ResTempP = Prediction[n, 2]\n",
        "    axs[0].text(txt_x, txt_y, \n",
        "                'Temp True: ' + '{:.0f}'.format(ResTempT), fontsize=12)\n",
        "    axs[0].text(txt_x, txt_y+160, \n",
        "                'Temp Pred: ' + '{:.0f}'.format(ResTempP), fontsize=12)\n",
        "    axs[0].set_ylim([0, 2000])\n",
        "\n",
        "    # plot pressure curve\n",
        "    axs[1].plot(XTest[n, :, 1], Depth_i, lw=lw, alpha=alpha, color='r', label=r'Log')\n",
        "\n",
        "    # plot reservoir pressure depth truth vs. prediction\n",
        "    ResPressureDepth = dx*YTest[n, 4]\n",
        "    axs[1].plot([np.min(XTest[n, :, 1]), np.max(XTest[n, :, 1])], \n",
        "                [ResPressureDepth,ResPressureDepth], lw=lw, alpha=alpha, \n",
        "                color='g', label=r'Truth')\n",
        "    ResPressureDepthPred = dx*Prediction[n, 4]\n",
        "    axs[1].plot([np.min(XTest[n, :, 1]), np.max(XTest[n, :, 0])], [ResPressureDepthPred,ResPressureDepthPred], lw=lw, alpha=alpha, color='b',linestyle='dashed', label=r'Prediction')\n",
        "\n",
        "    # plot reservoir pressure - truth vs prediction\n",
        "    ResPresT = YTest[n, 3]\n",
        "    ResPresP = Prediction[n, 3]\n",
        "    axs[1].text(txt_x, txt_y, 'Pres True: ' + '{:.1f}'.format(ResPresT), fontsize=12)\n",
        "    axs[1].text(txt_x, txt_y+160, 'Pres Pred: ' + '{:.1f}'.format(ResPresP), fontsize=12)\n",
        "\n",
        "    axs[0].invert_yaxis()\n",
        "    plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")\n",
        "\n",
        "    absErrorT =np.mean(np.abs(YTest[n, 0:3] - Prediction[n, 0:3]))\n",
        "    absErrorP =np.mean(np.abs(YTest[n, 3:] - Prediction[n, 3:]))\n",
        "\n",
        "    axs[0].set_xlabel(r'Temperature')\n",
        "    axs[0].set_ylabel('Depth')\n",
        "    axs[0].set_title('Temperature (mean err='+str(np.round(absErrorT,1))+')')\n",
        "    axs[1].set_xlabel(r'Pressure')\n",
        "    axs[1].set_title('Pressure (mean err='+str(np.round(absErrorP,1))+')')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkdUAH07kaMJ"
      },
      "source": [
        "# 4. Visualize trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv2JQfMbbtC9"
      },
      "source": [
        "for layer in model.layers:\n",
        "\t# check for convolutional layer\n",
        "\tif 'conv' not in layer.name:\n",
        "\t\tcontinue\n",
        "\t# get filter weights\n",
        "\tfilters, biases = layer.get_weights()\n",
        "\tprint(layer.name, filters.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsambeVxds1O"
      },
      "source": [
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[0].get_weights()\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "# plot first few filters\n",
        "n_filters, ix = 3, 1\n",
        "\n",
        "fig, axs = plt.subplots(3, 3, figsize=(3,12))\n",
        "\n",
        "for i in range(n_filters):\n",
        "    # get the filter\n",
        "    f = filters[:, :, i]\n",
        "    # plot each channel separately\n",
        "    for j in range(3):\n",
        "        # plot filter channel in grayscale\n",
        "        #ax = axs[i, j\n",
        "        axs[i,j].imshow(f[:, j].reshape((-1,1)), cmap='gray')\n",
        "        axs[i,j].axis('off')\n",
        "        axs[i,j].set_xticks([])\n",
        "        axs[i,j].set_yticks([])# show the figure"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56j-N9zzlGpn"
      },
      "source": [
        "## 4.1 see the features after they were convolved with these kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_H6uN34AkKXc"
      },
      "source": [
        "# redefine model to output right after the first hidden layer\n",
        "model_1 = Model(inputs=model.inputs, outputs=model.layers[0].output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbWYmjeQmkLs"
      },
      "source": [
        "feature_maps = model_1.predict(XTrain[0].reshape(1,200,3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD6sMvn5pO_1"
      },
      "source": [
        "# retrieve weights from the second hidden layer\n",
        "filters, biases = model.layers[0].get_weights()\n",
        "# normalize filter values to 0-1 so we can visualize them\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOyx151dpgS4"
      },
      "source": [
        "np.linspace(0,2000, 200)[0]-np.linspace(0,2000, 200)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFlZQf1CkuQR"
      },
      "source": [
        "fig, axs = plt.subplots(3, 3, figsize=(4,18))\n",
        "\n",
        "for i in range(3):\n",
        "    axs[i, 0].plot(XTrain[0, :, 0], np.linspace(0,2000, 200))\n",
        "    axs[i, 0].set_title('Temperature')\n",
        "    axs[i, 0].set_ylim([0, 2000])\n",
        "    axs[i, 0].invert_yaxis()\n",
        "\n",
        "    axs[i, 1].step(filters[:,0,i], np.linspace(0,2000, 200)[0:len(filters[:,0,0])])\n",
        "    axs[i, 1].set_title('filter 1,1')\n",
        "    axs[i, 1].set_ylim([0, 200])\n",
        "    axs[i, 1].invert_yaxis()\n",
        "\n",
        "    axs[i, 2].plot(feature_maps[0, :, i], np.linspace(0,2000, len(feature_maps[0, :, i])))\n",
        "    axs[i, 2].set_title('feature map')\n",
        "    axs[i, 2].set_ylim([0, 2000])\n",
        "    axs[i, 2].set_yticks([])\n",
        "    axs[i, 2].invert_yaxis()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgKE5INdxUA4"
      },
      "source": [
        "# One before last activity: a competition!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEuSzSAYPYUf"
      },
      "source": [
        "run_n=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si5GPJu_I7fA"
      },
      "source": [
        "## Step 1: play around with the hyper parameters and try to get the smallest validation error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU5EBbSXI5de"
      },
      "source": [
        "#set the ranges for the hyper-parameter space\n",
        "start = time.time()\n",
        "params= {'Conv1D_1_batchnorm': 'True', 'Conv1D_1_activation':'relu', 'Conv1D_1_maxpool': 'True', 'Conv1D_1_maxpool_padding': 'same', \n",
        "         'Conv1D_1_dropout': 'False', 'Conv1D_mid_padding':'same', 'Conv1D_mid_batchnorm':'True', 'Conv1D_mid_maxpool_padding': 'same',\n",
        "         'Conv1D_mid_dropout': 'True', 'Conv1D_mid_maxpool':'True', 'Conv1D_1_dropout_rate': 0.1, 'Conv1D_mid_dropout_rate': 0.1, \n",
        "         'decay': 0.0001, 'Conv1D_1_output_filters': 19, 'batch_size': 100, 'batch_size': 100, 'Conv1D_mid_kernel_size': 3}\n",
        "\n",
        "##########################\n",
        "# Discrete parameters\n",
        "# divided into parameters for the first block, middle blocks, and final block\n",
        "##########################\n",
        "\n",
        "# convolution filter edge treatment for first convolution block\n",
        "# 'Conv1D_1_padding':['valid', 'same']\n",
        "params['Conv1D_1_padding'] = 'valid'; \n",
        "\n",
        "# The activation used for the first convolution block\n",
        "# 'Conv1D_mid_activation': ['relu', 'sigmoid', 'tanh']\n",
        "params['Conv1D_mid_activation'] = 'relu'\n",
        "\n",
        "###############################\n",
        "# float values\n",
        "##############################\n",
        "\n",
        "# 'learning_rate':[0.001, 0.007]\n",
        "params['learning_rate'] = 0.003\n",
        "\n",
        "###############################\n",
        "# integer parameters\n",
        "# shown with the min and max recommended values\n",
        "##############################\n",
        "\n",
        "# The number of covolution blocks\n",
        "# 'num_conv_blocks': [2, 7]\n",
        "params['num_conv_blocks'] = 3\n",
        "\n",
        "# The kernel/filter size for the first convolutional block\n",
        "# 'Conv1D_1_kernel_size':[3, 30] \n",
        "params['Conv1D_1_kernel_size'] = 3\n",
        "\n",
        "# The number of filters in the middle convolution blocks\n",
        "# 'Conv1D_mid_output_filters': [3, 35]\n",
        "params['Conv1D_mid_output_filters'] = 16\n",
        "\n",
        "# the size of the kernel in the first maxpooling\n",
        "# 'Conv1D_1_maxpool_size':[2, 4]\n",
        "params['Conv1D_1_maxpool_size'] = 3\n",
        "\n",
        "# the size of the kernel in the middle blocks\n",
        "# 'Conv1D_mid_maxpool_size':[2, 4]\n",
        "params['Conv1D_mid_maxpool_size'] = 3\n",
        "\n",
        "# The filter size in the middle convolution block\n",
        "# 'Conv1D_mid_kernel_size': [3, 30]\n",
        "params['Conv1D_mid_kernel_size'] = 3\n",
        "\n",
        "print('you finished setting the hyper parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r25jk9rLPEMU"
      },
      "source": [
        "# determine the data input shape\n",
        "input_shape = XTrain.shape[1:]\n",
        "params[\"input_shape\"] = input_shape\n",
        "\n",
        "# build the model\n",
        "model = build_model(params)\n",
        "\n",
        "# create an optimizer object\n",
        "optimizer = keras.optimizers.Adam(lr=params['learning_rate'],\n",
        "                                  beta_1=0.9, \n",
        "                                  beta_2=0.999, \n",
        "                                  clipnorm=1, \n",
        "                                  decay=params['decay'])\n",
        "\n",
        "# compile the model and set the tracking parameters\n",
        "model.compile(optimizer = optimizer,\n",
        "              loss = \"mean_squared_error\",\n",
        "              metrics = [tf.keras.metrics.MeanAbsoluteError()])\n",
        "\n",
        "# view the model summary\n",
        "model.summary()\n",
        "\n",
        "run_n=run_n+1\n",
        "\n",
        "params['filename'] = 'myrun_'+str(run_n)\n",
        "    \n",
        "model_checkpoint = ModelCheckpoint(params['filename']+'.h5', \n",
        "                                  monitor='val_mean_absolute_error',\n",
        "                                save_best_only = True)\n",
        "\n",
        "log_dir = 'tb_mylogs/myrun_'+str(run_n)\n",
        "tb = TensorBoard(log_dir=log_dir)\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "\n",
        "\n",
        "hparams = {i:params[i] for i in params if i in ['Conv1D_1_padding', 'Conv1D_mid_activation', 'learning_rate', 'Conv1D_1_kernel_size', 'Conv1D_mid_output_filters', 'Conv1D_1_maxpool_size', 'Conv1D_mid_maxpool_size', 'num_conv_blocks', 'Conv1D_mid_kernel_size']}\n",
        "history = model.fit(x=XTrain, y=YTrain,\n",
        "                    batch_size=params['batch_size'],\n",
        "                    epochs=15, \n",
        "                    verbose=1, \n",
        "                    callbacks=[model_checkpoint, tb, hp.KerasCallback(log_dir, hparams, trial_id=params['filename'])], \n",
        "                    validation_data=(XVal, YVal), \n",
        "                    shuffle=True,\n",
        "                    class_weight=None, sample_weight=None, initial_epoch=0)\n",
        "\n",
        "## add the validation error to tensor board\n",
        "runName = str(params['filename']) + '.h5'\n",
        "new_model = load_model(runName)\n",
        "Prediction = new_model.predict(XVal)\n",
        "mean_abs_error = np.mean(np.abs(Prediction - YVal))\n",
        "tf.summary.scalar('validation_mae', data=mean_abs_error, step=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ_SgYU3Jeaj"
      },
      "source": [
        "%tensorboard --'logdir' 'tb_mylogs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjXoFT6YDubD"
      },
      "source": [
        "### Check out the testing error of different runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1BrbqX9LYw6"
      },
      "source": [
        "num = run_n\n",
        "runName = str('myrun_'+str(num)) + '.h5'\n",
        "new_model = load_model(runName)\n",
        "Prediction = new_model.predict(XTest)\n",
        "\n",
        "mean_abs_error = np.mean(np.abs(Prediction - YTest))\n",
        "print('Testing mean absolute error is: ' + str(mean_abs_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdbwJiGhLak_"
      },
      "source": [
        "# ONLY GO INTO THIS SECTION WHEN INSTRUCTOR SAYS TO:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i_Jt5YFxX3N"
      },
      "source": [
        "# get an image from my github\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/ahinoamp/CNN_temp_log_tutorial/main/TestingDataX1.csv\n",
        "\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/ahinoamp/CNN_temp_log_tutorial/main/TestingDataX2.csv\n",
        "\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/ahinoamp/CNN_temp_log_tutorial/main/TestingDataY.csv\n",
        "\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://raw.githubusercontent.com/ahinoamp/CNN_temp_log_tutorial/main/TestingDataX3.csv\n",
        "\n",
        "UnseenTestingDataX = np.zeros((4000, 200, 3))\n",
        "UnseenTestingDataX[:, :, 0] = np.genfromtxt('TestingDataX1.csv', delimiter=',')\n",
        "UnseenTestingDataX[:, :, 1] = np.genfromtxt('TestingDataX2.csv', delimiter=',')\n",
        "UnseenTestingDataX[:, :, 2] = np.genfromtxt('TestingDataX3.csv', delimiter=',')\n",
        "\n",
        "CompetitionY = np.genfromtxt('TestingDataY.csv', delimiter=',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxK8IfhDyqWU"
      },
      "source": [
        "num_best_model = 1\n",
        "modelname = 'myrun_'+str(num_best_model) + '.h5'\n",
        "best_model = load_model(modelname)\n",
        "Prediction = best_model.predict(UnseenTestingDataX)\n",
        "\n",
        "mean_abs_error = np.mean(np.abs(Prediction - CompetitionY))\n",
        "print('Your competition mean absolute error is: ' + str(np.round(mean_abs_error,1)).replace('.', 'pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts-xTtJNeZP3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}